{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.8"},"colab":{"name":"Face_identification.ipynb","provenance":[],"toc_visible":true}},"cells":[{"cell_type":"markdown","metadata":{"id":"dbdL6lwsmx8Z","colab_type":"text"},"source":["# Face identification"]},{"cell_type":"markdown","metadata":{"id":"P_Xg4i5Dmx8c","colab_type":"text"},"source":["### Inport needed libraries\n","\n","For clarity, and to avoid problems, firstly include all needed libraries at the begining of the notebook. Import all needed libraries."]},{"cell_type":"code","metadata":{"id":"HiOpUs2Rmx8g","colab_type":"code","colab":{}},"source":["# Import general purpose python libraries\n","import os\n","import matplotlib.pyplot as plt\n","from PIL import Image # For handling the images\n","import numpy as np\n","import math\n","\n","# Import different Keras functionalities\n","from keras.models import Sequential\n","from keras.layers import Dense\n","from keras.layers import Dropout\n","from keras.layers import Flatten\n","#from keras.layers import concatenate\n","#from keras.constraints import maxnorm\n","from keras.optimizers import SGD\n","from keras.layers.convolutional import Conv2D\n","from keras.layers.convolutional import MaxPooling2D\n","from keras.utils import np_utils\n","from keras import backend as K\n","K.common.image_dim_ordering()\n","\n","from keras.models import Model\n","from keras.layers import Input, Dense\n","import keras.backend.tensorflow_backend as K2\n","from keras.models import load_model\n","import tensorflow as tf\n","\n","from keras.applications.resnet50 import ResNet50\n","\n","from pathlib import Path\n","\n","import matplotlib.pyplot as plt\n","\n","from keras.preprocessing.image import ImageDataGenerator \n","\n","# Import function to plot the confussion matrix\n","#import plotcm"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FRPKqcUhmx8q","colab_type":"text"},"source":["### Database samples\n","\n","The face database is in folder \"MIT-CBCL\". Database images are split into \"train\", \"test\", and \"val\", folders. Each of those three folders is composed of 10 different folders, each of those folders contains images of each subject. Images do not have the same size but most of them are close to 150x150 pixels. With the next lines you can see a sample image of each subject in the training folder."]},{"cell_type":"code","metadata":{"id":"Iszkj-v1mx8s","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":232},"outputId":"f1cb3400-48c7-4f74-a5d7-f94981210dde","executionInfo":{"status":"error","timestamp":1577203118479,"user_tz":-60,"elapsed":1006,"user":{"displayName":"Maria Brull","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAlf9Wj2cIkz8I0xIM_gnFXMx7wdLhFh3LmnxBb=s64","userId":"01827463003857096616"}}},"source":["path_subjects = \"./MIT-CBCL/train\"\n","for i in os.listdir(path_subjects):\n","    count = 0\n","    for j in os.listdir(path_subjects + '/' + str(i)):\n","        if count==0:\n","            count = 1\n","            print(path_subjects + '/' + str(i) + '/' + str(j))\n","\n","            img = Image.open(path_subjects + '/' + str(i) + '/' + str(j))\n","            plt.imshow(img, cmap='gray', vmin=0, vmax=255)\n","            plt.title('subject: ' + str(i))\n","            plt.show()"],"execution_count":5,"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-5-da24a3c02537>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mpath_subjects\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"./MIT-CBCL/train\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_subjects\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_subjects\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './MIT-CBCL/train'"]}]},{"cell_type":"code","metadata":{"id":"jTa6VQXxoRr1","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":169},"outputId":"270b64ae-db9a-442e-ad3a-9429bb9487a8","executionInfo":{"status":"ok","timestamp":1577203111269,"user_tz":-60,"elapsed":23065,"user":{"displayName":"Maria Brull","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAlf9Wj2cIkz8I0xIM_gnFXMx7wdLhFh3LmnxBb=s64","userId":"01827463003857096616"}}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"7qWSCwMkmx80","colab_type":"text"},"source":["### Configuration parameters\n","Configuration values of different parts of the solution. You should change some of them to obtain better results."]},{"cell_type":"code","metadata":{"id":"omohkcggmx82","colab_type":"code","colab":{}},"source":["# Randomize the initial network weights\n","random_seed = True\n","\n","# Parameters that characterizes the images, size and image type\n","img_width = 150\n","img_height = 150\n","img_mode = \"grayscale\" #Load mode for images, either rgb or grayscale. In our case although some images could be rgb,\n","                       #we are going to work with grayscale images\n","\n","# Parameters that configures the training process\n","batch_size = 1 # Batch size\n","epochs = 1 # Number of epochs\n","initial_epoch = 0 # Initial epoch, it can be greater than 0 if you want to contiue a previous training process\n","initial_lr = 1e-10 # Learning rate\n","\n","# Paths to where training, testing, and validation images are\n","database_dir = './MIT-CBCL'\n","train_dir = './MIT-CBCL/train'\n","val_dir = './MIT-CBCL/val'\n","test_dir = './MIT-CBCL/test'\n","\n","# Directory where to store weights of the model and results\n","experiment_rootdir = \"./test/\"\n","# Create experiment directory if it does not exists\n","if not os.path.exists(experiment_rootdir):\n","    os.makedirs(experiment_rootdir)\n","\n","weights_path = \"weights.h5\" # Name of the file to store the weights\n","\n","# Output dimension (number of sublects in our problem)\n","num_classes = 10\n","\n","# Name of each gesture of the database\n","CLASSES = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yX7zCZtOmx86","colab_type":"text"},"source":["# Training process"]},{"cell_type":"markdown","metadata":{"id":"LTxACURUmx88","colab_type":"text"},"source":["### Create the model\n","Here you should introduce your solution for the model that obtains better results."]},{"cell_type":"code","metadata":{"id":"mOTN1ZdWmx89","colab_type":"code","colab":{}},"source":["def getModel(img_width, img_height, img_channels, output_dim, weights_path):\n","    \"\"\"\n","    Initialize model.\n","\n","    # Arguments\n","       img_width: Target image widht.\n","       img_height: Target image height.\n","       img_channels: Target image channels.\n","       output_dim: Dimension of model output (number of classes).\n","       weights_path: Path to pre-trained model.\n","\n","    # Returns\n","       model: A Model instance.\n","    \"\"\"\n","    \n","    # Define the input shape indicating the width, heigh, and depth of the images\n","    input_image = (img_width,img_height,img_channels)\n","    # Create the model itself\n","    restnet = ResNet50(include_top= True, weights= None, input_shape=input_image, classes=output_dim)\n","     \n","    #out= Dense(output_dim, activation='softmax')(restnet)\n","\n","    #model=model.add(Dense(512, activation='relu', input_dim=input_shape))\n","    #model = Model(inputs=input_image, output=out);\n","    \n","    # Load pretrained model if it exists\n","    if weights_path:\n","        try:\n","            restnet.load_weights(weights_path)\n","            print(\"Loaded model from {}\".format(weights_path))\n","        except:\n","            print(\"Impossible to find weight path. Returning untrained model\")\n","\n","    # Return the model itself\n","    return restnet"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3ymzYbXVmx9C","colab_type":"text"},"source":["### Set model training process\n","Includes the compiles, which you can modify, a callback to just save the model if the validation loss decreases, and fits the model."]},{"cell_type":"code","metadata":{"id":"q0QnuH1nmx9E","colab_type":"code","colab":{}},"source":["def trainModel(train_data_generator, val_data_generator, model, initial_epoch, initial_lr, experiment_rootdir, batch_size, epochs, weights_path):\n","    \"\"\"\n","    Model training.\n","\n","    # Arguments\n","       train_data_generator: Training data generated batch by batch.\n","       val_data_generator: Validation data generated batch by batch.\n","       model: A Model instance.\n","       initial_epoch: Epoch from which training starts.\n","       \n","    # Returns\n","        history: Model history\n","    \n","    \"\"\"\n","    \n","    # Configure the trainig process by compiling the model. Select the loss fucntion, the optimizer, and the metric used to obtain results.\n","    sgd = SGD(lr=initial_lr, momentum=0.9, decay=0, nesterov=False)\n","\n","    model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n","    #print(model.summary())\n","    \n","    # Define training and validation steps taking into account the number of samples for each process and the batch size\n","    steps_per_epoch = math.floor(train_data_generator.samples / batch_size)\n","    validation_steps = math.floor(val_data_generator.samples / batch_size)\n","        \n","    # Fit the model by using the fit generator\n","    model.fit_generator(train_data_generator,steps_per_epoch= steps_per_epoch, validation_data=val_data_generator,  validation_steps=validation_steps, epochs = epochs)\n","    \n","    # Return the history of the model to plot the loss and accuracy evolution\n","    history = model.fit_generator(train_data_generator, val_data_generator, epochs = epochs)\n","    \n","    \n","                "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"OqQ7mSwjmx9H","colab_type":"code","colab":{},"outputId":"8b372eb7-bb45-486a-a929-27242063ce9c"},"source":["math.floor(train_generator.samples / batch_size)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1200"]},"metadata":{"tags":[]},"execution_count":33}]},{"cell_type":"markdown","metadata":{"id":"KjnTQwjvmx9L","colab_type":"text"},"source":["### Load training and validation data\n","Loads training and validation data in a DataGenerator which divides the data in batches and prepares it for the training process."]},{"cell_type":"code","metadata":{"id":"1hpHCeMwmx9M","colab_type":"code","colab":{},"outputId":"a34c61e4-75ff-41dc-c13b-c22cebdd9e78"},"source":["# Set random seed\n","if random_seed:\n","    seed = np.random.randint(0,2*31-1)\n","else:\n","    seed = 5\n","np.random.seed(seed)\n","tf.set_random_seed(seed)\n","\n","# Select the number of channels of the image considering the image mode (RGB or grayscale)\n","if img_mode=='rgb':\n","    img_channels = 3\n","elif img_mode == 'grayscale':\n","    img_channels = 1\n","else:\n","    raise IOError(\"Unidentified image mode: use 'grayscale' or 'rgb'\")\n","\n","# Create train_datagenerator using ImageDataGenerator of keras.\n","\n","train_data_generator = ImageDataGenerator(featurewise_center=True, featurewise_std_normalization=True, rescale =1./255)\n","\n","# Iterator object containing training data to be generated batch by batch\n","\n","train_generator=train_data_generator.flow_from_directory(train_dir, target_size=(150, 150), color_mode='grayscale')\n","\n","# Check if the number of classes in dataset corresponds to the one specified                                                    \n","assert train_generator.num_classes == num_classes, \\\n","                    \" Not macthing output dimensions in training data.\"                                                    \n","\n","\n","# Create val_datagenerator using ImageDataGenerator of keras.\n","\n","val_data_generator = ImageDataGenerator(featurewise_center=True, featurewise_std_normalization=True, rescale =1./255)\n","\n","# Iterator object containing validation data to be generated batch by batch\n","\n","val_generator=val_data_generator.flow_from_directory(val_dir, target_size=(150, 150), color_mode='grayscale')\n","\n","# Check if the number of classes in dataset corresponds to the one specified\n","assert val_generator.num_classes == num_classes, \\\n","                    \" Not macthing output dimensions in validation data.\""],"execution_count":0,"outputs":[{"output_type":"stream","text":["Found 1200 images belonging to 10 classes.\n","Found 400 images belonging to 10 classes.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ZQuBlZYpmx9S","colab_type":"text"},"source":["### Obtain and train the model itself\n","Load the model that you have specificly created and trains it"]},{"cell_type":"code","metadata":{"id":"q-VJfWDWmx9U","colab_type":"code","colab":{},"outputId":"81ade6ec-79cf-416b-a905-8cc92ac3f32d"},"source":["# Create the model by using the getModel function\n","trained_model=getModel(img_width, img_height, img_channels, num_classes, weights_path)\n","\n","# Train the model by using the trainModel function\n","trained_model=trainModel(train_generator, val_generator, trained_model, initial_epoch, initial_lr, experiment_rootdir, batch_size, epochs, weights_path)\n","\n","# Save weights that can be used in future training process\n","weights_save_path = os.path.join(experiment_rootdir, weights_path)\n","trained_model.save_weights(weights_save_path)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Impossible to find weight path. Returning untrained model\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/image_data_generator.py:716: UserWarning: This ImageDataGenerator specifies `featurewise_center`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n","  warnings.warn('This ImageDataGenerator specifies '\n","/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/image_data_generator.py:724: UserWarning: This ImageDataGenerator specifies `featurewise_std_normalization`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n","  warnings.warn('This ImageDataGenerator specifies '\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch 1/1\n","1200/1200 [==============================] - 383s 319ms/step - loss: 3.2246 - accuracy: 0.0427 - val_loss: 3.3287 - val_accuracy: 0.0574\n","Epoch 1/1\n"],"name":"stdout"},{"output_type":"error","ename":"TypeError","evalue":"'<' not supported between instances of 'int' and 'DirectoryIterator'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-48-74a6cb16402b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Train the model by using the trainModel function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtrained_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_generator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_generator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrained_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_lr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexperiment_rootdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Save weights that can be used in future training process\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-46-9ecf670e1d1f>\u001b[0m in \u001b[0;36mtrainModel\u001b[0;34m(train_data_generator, val_data_generator, model, initial_epoch, initial_lr, experiment_rootdir, batch_size, epochs, weights_path)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;31m# Return the history of the model to plot the loss and accuracy evolution\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data_generator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_data_generator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1730\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1731\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1732\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1733\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    182\u001b[0m             \u001b[0msteps_done\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m             \u001b[0mbatch_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 184\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0msteps_done\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    185\u001b[0m                 \u001b[0mgenerator_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: '<' not supported between instances of 'int' and 'DirectoryIterator'"]}]},{"cell_type":"markdown","metadata":{"id":"KbwmMfr4mx9Y","colab_type":"text"},"source":["# Testing"]},{"cell_type":"markdown","metadata":{"id":"HqRh0fxjmx9a","colab_type":"text"},"source":["### Predictions computation\n","Function to obtain the predictions over the testing data, it also outputs the ground truth of the input data"]},{"cell_type":"markdown","metadata":{"id":"VGYtBVtXmx9c","colab_type":"text"},"source":["### Load testing data"]},{"cell_type":"code","metadata":{"id":"y7oJUGUZmx9d","colab_type":"code","colab":{},"outputId":"b4f719e2-b68d-459f-8d6f-767b69ad4f45"},"source":["# Create test_datagenerator using ImageDataGenerator of keras.\n","test_data_generator = ImageDataGenerator(featurewise_center=True, featurewise_std_normalization=True, rescale =1./255)\n","\n","# Iterator object containing validation data to be generated batch by batch\n","test_generator=test_data_generator.flow_from_directory(test_dir, target_size=(150, 150), color_mode='grayscale')\n","\n","# Check if the number of classes in dataset corresponds to the one specified\n","assert test_generator.num_classes == num_classes, \\\n","                    \" Not macthing output dimensions in test data.\""],"execution_count":0,"outputs":[{"output_type":"stream","text":["Found 400 images belonging to 10 classes.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"iq4Ibkg4mx9g","colab_type":"text"},"source":["### Testing process\n","Load the model, load the weight obtained by the training process, obtain testing results and plot those results in a confusion matrix."]},{"cell_type":"code","metadata":{"id":"LH_bG8iBmx9h","colab_type":"code","colab":{},"outputId":"71630596-3b3f-4766-ba72-4a81b0840335"},"source":["# Create the model by using the getModel function\n","test_model=getModel(img_width, img_height, img_channels, num_classes, weights_path)\n","\n","\n","# Load saved weights\n","weights_load_path = os.path.join(experiment_rootdir, weights_path)\n","\n","try:\n","    test_model.load_weights(weights_load_path)\n","    print(\"Loaded model from {}\".format(weights_load_path))\n","except:\n","    print(\"Impossible to find weight path. Returning untrained model\")\n","\n","\n","# Compile model\n","test_model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n","\n","# Get predictions using predict_generator function\n","predicted_values=predict_generator(test_generator, steps=test_generator.samples//batch_size)\n","\n","# Select the class which has the higher predicted value\n","max_class= max(predicted_values)\n","\n","# Create groundtruth\n","gt = np.zeros(pred_labels.shape)\n","init_index = 0\n","end_index = 0\n","user_id = 0\n","for users in sorted(os.listdir(test_dir)):\n","    user_id = user_id + 1\n","    if os.path.isdir(os.path.join(test_dir, users)):\n","        user_path = os.path.join(test_dir, users)\n","        for root, _, files in sorted(os.walk(user_path)):\n","            num_images_user = len(files)\n","            end_index = init_index + num_images_user\n","            gt[init_index:end_index] = user_id - 1\n","            init_index = end_index\n","            \n","# Evaluate predictions: Average accuracy and highest errors\n","print(\"-----------------------------------------------\")\n","print(\"Evaluation:\")\n","# Compute average accuracy\n","ave_accuracy = metrics.accuracy_score(gt, pred_labels)\n","print('Average accuracy = ', ave_accuracy)\n","print(\"-----------------------------------------------\")\n","\n","# Visualize confusion matrix                                           \n","plotcm.plotcm(experiment_rootdir, gt, pred_labels,CLASSES, experiment_rootdir, normalize=True)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Impossible to find weight path. Returning untrained model\n","Impossible to find weight path. Returning untrained model\n"],"name":"stdout"},{"output_type":"error","ename":"NameError","evalue":"name 'sgd' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-50-9cf2a357afed>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# Compile model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mtest_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'categorical_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msgd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# Get predictions using predict_generator function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'sgd' is not defined"]}]},{"cell_type":"markdown","metadata":{"id":"lteABGaLmx9k","colab_type":"text"},"source":["### Plot history for accuracy for training and validation process"]},{"cell_type":"code","metadata":{"id":"h4wCsKj8mx9l","colab_type":"code","colab":{}},"source":["plt.plot(history.history['accuracy'])\n","plt.plot(history.history['val_accuracy'])\n","plt.title('model accuracy')\n","plt.ylabel('accuracy')\n","plt.xlabel('epoch')\n","plt.legend(['train', 'test'], loc='upper left')\n","\n","# Save the figure\n","plt.savefig(accuracy_img_name)\n","\n","# Show figure\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CYS4n8UTmx9o","colab_type":"text"},"source":["### Plot history for loss for training and validation process"]},{"cell_type":"code","metadata":{"id":"9jtp2Vxemx9p","colab_type":"code","colab":{}},"source":["plt.plot(history.history['loss'])\n","plt.plot(history.history['val_loss'])\n","plt.title('model loss')\n","plt.ylabel('loss')\n","plt.xlabel('epoch')\n","plt.legend(['train', 'test'], loc='upper left')\n","\n","# Save the figure\n","plt.savefig(loss_img_name)\n","\n","# Show figure\n","plt.show()"],"execution_count":0,"outputs":[]}]}